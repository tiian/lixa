Test the behavior for MariaDB 10.5 using Rocky9 distro



lixa_my_open generates several memory leaks (LIXA_PROFILE=MYS_STA) but they 
have not been introduced by the new lixa_my_retrieve_subtype...
Are they related to the lixa_sw_status hash table???

Update year copyright
Update manual: remove versions older than Ubuntu 16.04 & MySQL 5.7




Under very high workload the state server stops with a controlled shutdown;
tracing is not viable because with active tracing the workload is lower and the
issue does not happen.
Investigate the topic as described below.

Find a match that crashes the lixad; this 
/opt/lixa/bin/lixat -b -p 500 -P 250 -l 100 -C 10000
generates this error:
May 24 20:58:27 ubuntu003 lixad[13969]: LXD018C internal thread id=2 raised unexpected exception (server_manager_thread: excp=10,ret_cod=-13,errno=11); immediate shutdown will terminate the server
even with new POLLNVAL handling in server_manager_thread.
even with new POLLNVAL handling in lixa_msg_receive.

Sometimes, interrupting the benchmark, lixad through an exception, even with default paratemeters

This config seems to "incentivate" the error: the larger the buffer, the slower
the write operation
  min_elapsed_sync_time="0" max_elapsed_sync_time="10"
  log_size="524288000" max_buffer_log_size="131072000"
  log_o_direct="1" log_o_dsync="0" log_o_rsync="0" log_o_sync="0">


Sometimes there's even this message very near the previous one:
May 25 20:15:56 ubuntu003 lixad[9391]: LXD058N father thread waited table flusher thread for 41 ms; storage is not properly tuned for the real workload
Some ideas:
It seems to happen after many interactions: dump the content of the state to
check how many "dirty" transactions are inside it.
Check the log switch: it seems to switch too many times.

Implement auto-migration from old state files to new state tables:
- if cold start & state files exists => migrate and switch
- if cold start & state files does not exist => cold start new state tables
- if warm start => ignore old state files

lixad restart is slow for journal state engine when logs are too large:
- even with a large buffer, lixad can ask up to 133 IOPS and 1.94 GB/s; with
large log files it can require several seconds to restart
- "server_manager" function activates the server threads sequentially: going parallel could be an option only for very high speed storage, but very high speed storage should not show a slow restart...
It's not clear how much performance benefit can be obtained following this path.

Clean memory leaks by mean of valgrind

Introduce a clean up policy in lixa_state_table_copy_from(): all the failed
transactions that are older than T1 and all the recovery pending transactions 
that are older than T2 will not be copied in the target state table

**********

Remove ext subdir for PHP old stuff

Implementation of xa_recover (useful for the future javarj):
- an example of receiving arrays of objects from Java to JNI: CallObjectMethod    https://github.com/joeferner/node-java/blob/master/src/utils.cpp
- how xid is implemented by pgjdbc :( https://github.com/pgjdbc/pgjdbc/blob/master/pgjdbc/src/main/java/org/postgresql/xa/RecoveredXid.java
-  

**********

Implement XTA for PHP, remove "ext" legacy code, update documentation

Put all configuration in XTA without necessity to load lixac_conf.xml file

Replace IP address with machine_id in lixa_job_set_source_ip to get unique ID
for every system partecipanting in transactions.
As an example see function flom_tls_get_unique_id from flom project:
https://github.com/tiian/flom/blob/master/src/flom_tls.c
The dbus_get_local_machine_id() function is easy to use, but requires dbus
installed.

Disk sync latency matters and XTA amplifies it: a different journaling strategy
should be implemented. See doc/improvements.txt
Explore high availability features (active/active) with dynamic or static
workload. Avoid disk persistence, prefer distributed memory semi persistence.

Implement an utility feature to check disk synchronization latency and report
it.

Supply Dockerfile to build a Docker container image.

Resource Managers suggested:
RabbitMQ (Jim Doyle)
Berkeley DB - Sleepycat (Jim Doyle)
RVM http://www.coda.cs.cmu.edu/doc/html/rvm_manual.html (Jim Doyle) 

When tx_rollback() is called, LIXA does not call xa_end(TMFAIL), but xa_end(TMSUCCESS); this is not
an error, but it does not inform the resource manager as soon as possible of the transaction manager
next action; try to change this piece of code of lixa_xa.c:
xa_end_flags=TMSUCCESS; (line 504)
with something like
if (commit)
  xa_end_flags=TMSUCCESS;
else
  xa_end_flags=TMFAIL;
A full regression test must be performed...



Try this scenario:
- a multithread application
- one thread connects to a resource manager like Oracle or DB2 using tx_open()
- after tx_open() the server crashes, for example in server_xa_start_24
- the thread calls tx_commit() after some native SQL update operations
- the server crashed, the thread receive TX_FAIL from the transaction manager
- the thread can not close the connection to the database using tx_close()
  because after TX_FAIL the transaction manager can no longer perform work on
  behalf of the application
- what happens with locks inside the database(s)?
- is it right to document the application must close the resource managers by
  itself after a TX_FAIL return code??
- is it sufficient thread termination to clean-up database(s) state(s)?
- generally tx_close() can not be executed: some times it may happen, some 
  times it can not work properly

Document a memory leak introduced by libxml2:
xmlInitParser()
xmlCleanupParser()
must be called from the same thread (cite the thread/libxml2 URL).
If xmlCleanupParser() is called from a different thread, 1 block of 24 bytes
is definitely lost.
It seems the leak is limited to 24 bytes, but some critical pattern might
exploit a recursive behavior... :(
This could be an issue for some situations:
if the called functions are executed inside threads and the main program is the
transaction monitor itself, there is no way to assure the same thread called
xmlInitParser() is able to call xmlCleanupParser() too because the first thread
should remain locked until all the thread completed.
From a LIXA point of view, xmlInitParser() is called from the first thread
that calls tx_open() and xmlCleanupParser() is called from the last thread
that leaves tx_close(). The issue must be discussed with libxml2 development
team. Unfortunately, tx_open() can not be called from the main program and the
following functions from a different thread because this behavior would violate
XA standard: to avoid this issue a "proxy thread" should be used for XA
functions, but the "solution" is worst then the "problem".
If the multithreaded application has a main program - you are not writing a
library or a module - you can call
tx_open() in the main, before every thread is started
tx_close() in the main, after every thread is completed
this should avoid the memory leak. Pay attention the tx_open() at main level
is not inherited at the thread level: every thread must call its own tx_open()
too; this is only a workaround to force xmlInitParser() and xmlCleanupParser()
only at main program instead of at first starting thread / last ending thread.
