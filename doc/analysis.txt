This document is a draft and contains ideas, issues, possible solutions.
Nothing else then work in progress stuff...

******************************************

JOB and PROFILE

The PROFILE concept describes a set of resource managers and the way they
must be reached. Example: network addresses and ports, switch load files must
be used, users & passwords for authentication and so on...

The JOB concept is related to the logical container of a set of transactions.
Examples.
The default JOB associated to a transaction is the fully qualified path name
of the program is executing the JOB.
If the JOB crashes, the recovery will happen only when the same JOB will
start again.
If you are working in a cluster environment, probably you need some flexibility
and the default JOB may be something related to "program name", but not
depending on "directory name". It maybe.
If you are working in some configuration with many different program 
supplying a service, probably you may be interested in specifying a logical
label instead of accepting the default program name. Imagine you can set-up
an environment variable, like this:
      export LIXA_JOB_NAME="payroll"
and then programs "pay01", "pay02", "pay54", "pay55" are all executed in an
environment with the variable set-up. Every job can recover the data of a
previous one.
If your transactions are inside "apache", you may likely leave "apache" as
the default JOB name.

This concept moves a lot of responsability on Lixa client library, relegating
the server to the only precious task of available and secure transaction
"register".
One consequence of this idea is the long term of transaction status inside
the server: a failed transaction could be "silently managed" for "infinite"
time.
The server can NOT use a linear journal, but a different non circular 
structure.

******************************************

RECOVERY PENDING status

A transaction enters "recovery pending" status when two different circumstances
happen:
- the client breaks the connection: the link between the Application Program
  and the Transaction Manager breaks, the in-flight transaction currently
  executed by the Application Program becomes "recovery pending"
- the server crashes: at restart, all the transactions was in-flight at
  crash time, are marked as "recovery pending"

Due to the intrinsic parallelism of client activities there could be many
transaction running with the same JOB attribute, inside different server
thread.
This is OK until a recovery pending status happens.
To recover a transaction on the client side, is NECESSARY, the same Application
Program from the same host, comes back and contact the Transaction Manager.
Apache may be a good example.
This is the scenario:
Apache is configured with max 20 servers.
The Transaction Manager is configured with 4 threads.
Apache is fully loaded, 20 servers are running, 20 Application Programs are
running, 20 transactions are in-flight.
From a Transaction Manager perspective, imagine every thread is serving 5
apache Application Program clients.
One Application Program crashes and breaks TCP/IP connection.
The Transaction Manager flags the in-flight transaction as "RECOVERY PENDING".
Another Application Program running inside Apache contacts the Transaction
Manager.
The listener chooses the thread with the minimum load: it may be different
from the thread is keeping the "RECOVERY PENDING" transaction.
To avoid races conditions and statistical locks (the right job is served by
the wrong thread forever...), the first thing a thread must check for an 
incoming client is:
1. check if I (thread) have got a "RECOVERY PENDING" transaction for this job
   - answer = NO, go 2
   - answer = YES: perform recovery and go 2
2. check in a shared table if any thread has "RECOVERY PENDING" transaction
for this job
   - answer = NO, go 3
   - answer = YES, create a message and route the client connection to the
                   first thread has a RECOVERY PENDING transaction
3. perform Application Program request...

These are true:
- if a thread has more than one transaction in recovery pending status, the
  thread has to recover all the transaction in recovery pending status (this
  happens due to point 1, it's an implicit loop)
- recovery may happen in parallel if many Application Programs come "at the
  same time": every thread will process one recovery pending transactions
  atomically before another one can be performed. This should avoid last
  arrivals start to process normal activity before all the recovery pending
  transactions are in charge. It may happen normal activity will start before
  the last recovery pending transaction has been recovered, but not before it
  started. This behaviour can produce some extra lock in Resource Managers
  and some extra rollback due to lock will disappear later...

The shared table must be protected by a mutex to avoid contention between
threads.
The search keys are:
"job": are there recovery pending transactions for this job?
"job+thread": are there recovery pending transactions for this job I have to
              perform?

These rules must be applied to shared table:
- Transaction Manager crash: at restart, every thread insert a record for
  every in-flight transaction inside the table
- Application Program crash: the serving thread insert a record if there was
  an in-flight transaction for the broken connection
- Thread recovers a transaction: at recovery time start, the record is removed
  from the shared table; it's responsability of the thread to complete the 
  recovery or insert a new record...  

******************************************

TRANSACTION MANAGERS, RESOURCE MANAGERS and PROFILES

All the information related to the RESOURCE MANAGERS is stored in client
configuration. 
A client application use a PROFILE to:
- specify the resource manager parameters (switch load file, open options,
  close options, etc...)
- specify the transaction manager will help the client to perform the XA
  stuff
The relation between PROFILES and TRANSACTION MANAGERS is N:1
A profile can be associated to only one transaction manager
A transaction manager can be associated to many profiles

The lixac_conf.xml file must radically change:
1. <trnmgr> tag must be enriched with an ID property
2. <trnmgr> tag must not contain a PROFILE property
3. <trnmgr> tags must be enclosed inside <trnmgrs> tag
4. <profiles> tag must be created
5. <profile> tags must be created inside <profiles>
6. <profile> tag must store switch load file and resource manager stuff

All these changes are necessary due to the different strategy: the server
becomes a "passive" component used only to manage XA status of all the 
managed transactions in a centralized manner. All the information needed to
interface the resource managers are stored in client side.
This prevent a double configuration issue but exposes to different issues
like: every client must be correctly configured to reach all the resource
managers.
Some information on server side would be in the wrong place:
- switch load file is a property of the system is hosting the client (the
  module could be installed everywhere)
- resource manager open strings many times contain user/password related to
  access credentials: these data are already stored client side because they
  are used by the client application. Why should we store them server side too?
- resource manager trace options can be very different from system to system
  (disk space, number of transaction performed, and so on...). Why should we
  centralize and risk to break a system only because another one need more
  tracing messages?

******************************************

PROFILE, PROCESS, THREADS

All the THREADS inside a PROCESS use the same PROFILE.
There's no way to specify different PROFILEs from distinct THREADs of the same
PROCESS.
This is partially due to environment variable LIXA_PROFILE: an environment var
is a PROCESS property, not a THREAD property.

******************************************

STATUS files

To preserve integrity the status file of every thread must be mirrored and
updated with an "integrity preserving" protocol.

These facts are true:
- memory mapped pages can be transferred to disk before synch is requested
- power supply can vanish in the middle of a disk block writing

These protocol is safe:
- there are two physical files for every logical status file
- every file is an array of records
- every record is delimited by two special fields:
  - record head is a 32 bit unsigned integer
  - record tail is a 16 bytes array 

Initial status: file 0 is OK, just synched to disk
Copy all changed blocks to file 1.
Use file 1.
Every time a record must be modified, check the head special field:
  if (! head % 2)
    head++;
  Explanation:
  - if head is even, it's yet unmodified, increment it
  - if head is odd, it's already modified, keep it
  then add the record in the changed block list
When a synch of file 1 must be performed, do:
  for every modified block
    head++;  /* if head is already even, this is an internal error!! */
    compute MD5 digest of the block without tail special field
    put MD5 digest in tail special field
  synch file 1
  copy all changed blocks to file 0
  Use file 0
 
[... and so on ...]

At server restart, the two images must be analyzed in parallel.
Starting with some record, one of the two may be non integral.

These conditions may happen:
- 2 integral (identical) files: server shutdown happened without issues
- 1 integral file: server crashed, but the integral file is the good one and
    transaction recovery can be performed
- 0 integral files: critical situation
    - a software bug
    - someone damaged at least one file

Corollary: the initial formatting must sign with MD5 digest all records before
usage.

MD5 digest cost: few microseconds, 20% of a TRACE(( )) statement

Interesting thing: this crash safe status saving does not require a double
                   sync on disk, the worst case in terms of performances
