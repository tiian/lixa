This document is a draft and contains ideas, issues, possible solutions.
Nothing else then work in progress stuff...

******************************************

JOB and PROFILE

The PROFILE concept describes a set of resource managers and the way they
must be reached. Example: network addresses and ports, switch load files must
be used, users & passwords for authentication and so on...

The JOB concept is related to the logical container of a set of transactions.
Examples.
The default JOB associated to a transaction is composed as:

source_IP_address|md5digest(lixac_real_path||profile)|substr(profile,0,32)|\0
      15                          32                       16               1

lixac_real_path: pathconf("/", _PC_PATH_MAX) to allocate buffer
realpath("/etc/lixac_conf.xml", buffer) to retrieve the real filename

this identifier allows to distinguish clients arriving from the same system,
using the same profile, but pointing to different config files (and the
resource managers associated might be different!)
If the JOB crashes, the recovery will happen only when the same JOB will
start again.
Many situations may need greater flexibility; this can be added with an
environment variable like this:
      export LIXA_JOB_NAME="payroll"
if you set this environment var before the execution of the 
programs "pay01", "pay02", "pay54", "pay55" every job can recover the data of a
previous one.
If your transactions are inside "apache", you may likely set "apache" as
the default JOB name or "apache@server1" and "apache@server2" to distinguish
two different transactional logical container.

The concept of JOB is really important for recovery actions: the Transaction
Manager is generally an active component, but not in LIXA where the
TM is only the actor coordinates the activities, not the actor performs the
activities.
If a JOB does not present itself to the TM for a long time, previous
recovery pending transactions will remain parked inside TM status.
If for any reason, the wrong client, try to recover a transaction, it may
happen it's pointing different resource managers: they will ask the transaction
is not available, and everything can only go worst...
This is one of the most powerful and most critical issue of LIXA technology:
the Transaction Manager is completely decoupled from the Transaction Monitor.
Every Transaction Monitor - the shell itself - can perform two phase commit
transactional tasks with the support of LIXA: this is extremely powerful.
On the dark side, there is some potential risk if the configuration is wrong:
the Transaction Manager can ask a client to start the recovery of a 
transaction should be recovered in a different location... This is extremely
dangerous and must be strongly avoided!

This concept moves a lot of responsability on Lixa client library, relegating
the server to the only precious task of available and secure transaction
"register".
One consequence of this idea is the long term of transaction status inside
the server: a failed transaction could be "silently managed" for "infinite"
time.

******************************************

RECOVERY PENDING status

A transaction enters "recovery pending" status when two different circumstances
happen:
- the client breaks the connection: the link between the Application Program
  and the Transaction Manager breaks, the in-flight transaction currently
  executed by the Application Program becomes "recovery pending"
- the server crashes: at restart, all the transactions was in-flight at
  crash time, are marked as "recovery pending"

Due to the intrinsic parallelism of client activities there could be many
transaction running with the same JOB attribute, inside different server
thread.
This is OK until a recovery pending status happens.
To recover a transaction on the client side, is NECESSARY, the same Application
Program from the same host, comes back and contacts the Transaction Manager.
Apache may be a good example.
This is the scenario:
Apache is configured with max servers = 20.
The Transaction Manager is configured with 4 threads.
Apache is fully loaded, 20 servers are running, 20 Application Programs are
running, 20 transactions are in-flight.
From a Transaction Manager perspective, imagine every thread is serving 5
apache Application Program clients.
One Application Program crashes and breaks TCP/IP connection.
The Transaction Manager flags the in-flight transaction as "RECOVERY PENDING".
Another Application Program running inside Apache contacts the Transaction
Manager.
The listener chooses the thread with the minimum load: it may be different
from the thread is keeping the "RECOVERY PENDING" transaction.
To avoid race conditions and statistical locks (the right job is served by
the wrong thread forever...), the first thing a thread must check for an 
incoming client is:
1. check if I (thread) have got a "RECOVERY PENDING" transaction for this job
   - answer = NO, go 2
   - answer = YES: perform recovery and go 2
2. check in a shared table if any thread has "RECOVERY PENDING" transaction
for this job
   - answer = NO, go 3
   - answer = YES, create a message and route the client connection to the
                   first thread has a RECOVERY PENDING transaction
3. perform Application Program request...

These are true:
- if a thread has more than one transaction in recovery pending status, the
  thread has to recover all the transaction in recovery pending status (this
  happens due to point 1, it's an implicit loop)
- recovery may happen in parallel if many Application Programs come "at the
  same time": every thread will process one recovery pending transactions
  atomically before another one can be performed. This should avoid last
  arrivals start to process normal activity before all the recovery pending
  transactions are in charge. It may happen normal activity will start before
  the last recovery pending transaction has been recovered, but not before it
  started. This behaviour can produce some extra lock in Resource Managers
  and some extra rollback due to lock will disappear later...

The shared table must be protected by a mutex to avoid contention between
threads.
The search keys are:
"job": are there recovery pending transactions for this job?
"job+thread": are there recovery pending transactions for this job I have to
              perform?

These rules must be applied to shared table:
- Transaction Manager crash: at restart, every thread insert a record for
  every in-flight transaction inside the table
- Application Program crash: the serving thread insert a record if there was
  an in-flight transaction for the broken connection
- Thread recovers a transaction: at recovery time start, the record is removed
  from the shared table; it's responsability of the thread to complete the 
  recovery or insert a new record...  

Shared table implementation:
- first level : GMutex: used to protect the access to the table itself
- first level = GTree (balanced binary trees): used to look-up job string
- second level = GArray (array): used to look-up thread id after the job has
                 been resolved
- third level = GQueue (double ended queue): used to iterate over all the
                blocks (transactions) must be recovered
The recovery pending table is a 3 levels structure (cube) must accessed 
following this order: job thread block
It must be realized in an object oriented fashion because it's used in 
completely different scopes:
- server restart: the table is created from scratch analyzing status file
- client crash: the table is update with a new record
- client arrival: the table is consulted and updated if necessary (recovery
                  performed)

******************************************

TRANSACTION MANAGERS, RESOURCE MANAGERS and PROFILES

All the information related to the RESOURCE MANAGERS is stored in client
configuration. 
A client application use a PROFILE to:
- specify the resource manager parameters (switch load file, open options,
  close options, etc...)
- specify the transaction manager will help the client to perform the XA
  stuff
The relation between PROFILES and TRANSACTION MANAGERS is N:1
A profile can be associated to only one transaction manager
A transaction manager can be associated to many profiles

The lixac_conf.xml file must radically change:
1. <trnmgr> tag must be enriched with an ID property
2. <trnmgr> tag must not contain a PROFILE property
3. <trnmgr> tags must be enclosed inside <trnmgrs> tag
4. <profiles> tag must be created
5. <profile> tags must be created inside <profiles>
6. <profile> tag must store switch load file and resource manager stuff

All these changes are necessary due to the different strategy: the server
becomes a "passive" component used only to manage XA status of all the 
managed transactions in a centralized manner. All the information needed to
interface the resource managers are stored in client side.
This prevent a double configuration issue but exposes to different issues
like: every client must be correctly configured to reach all the resource
managers.
Some information on server side would be in the wrong place:
- switch load file is a property of the system is hosting the client (the
  module could be installed everywhere)
- resource manager open strings many times contain user/password related to
  access credentials: these data are already stored client side because they
  are used by the client application. Why should we store them server side too?
- resource manager trace options can be very different from system to system
  (disk space, number of transaction performed, and so on...). Why should we
  centralize and risk to break a system only because another one need more
  tracing messages?

******************************************

PROFILE, PROCESS, THREADS

All the THREADS inside a PROCESS use the same PROFILE.
There's no way to specify different PROFILEs from distinct THREADs of the same
PROCESS.
This is partially due to environment variable LIXA_PROFILE: an environment var
is a PROCESS property, not a THREAD property.

******************************************

STATUS files

To preserve integrity the status file of every thread must be mirrored and
updated with an "integrity preserving" protocol.

These facts are true:
- memory mapped pages can be transferred to disk before synch is requested
- power supply can vanish in the middle of a disk block writing

This protocol is safe:
- there are two physical files for every logical status file
- every file is an array of records
- every record is delimited by two special fields:
  - record head is a 32 bit unsigned integer
  - record tail is a 16 bytes array 

Initial status: file 0 is OK, just synched to disk
Copy all changed blocks to file 1.
Use file 1.
Every time a record must be modified, check the head special field:
  if (! head % 2)
    head++;
  Explanation:
  - if head is even, it's yet unmodified, increment it
  - if head is odd, it's already modified, keep it
  then add the record in the changed block list
When a synch of file 1 must be performed, do:
  for every modified block
    head++;  /* if head is already even, this is an internal error!! */
    compute MD5 digest of the block without tail special field
    put MD5 digest in tail special field
  synch file 1
  copy all changed blocks to file 0
  Use file 0
 
[... and so on ...]

At server restart, the two images must be analyzed in parallel.
Starting with some record, one of the two may be non integral.

These conditions may happen:
- 2 integral (identical) files: server shutdown happened without issues
- 1 integral file: server crashed, but the integral file is the good one and
    transaction recovery can be performed
- 0 integral files: critical situation
    - a software bug
    - someone damaged at least one file

Corollary: the initial formatting must sign with MD5 digest all records before
usage.

MD5 digest cost: few microseconds, 20% of a TRACE(( )) statement

Interesting thing: this crash safe status saving does not require a double
                   sync on disk, the worst case in terms of performances

******************************************

TRANSACTION UNIQUE IDENTIFIERS

A 16 bytes unique id can be generated with:
uuidgen (command line)
libuuid (program; probably uuid_generate_time uuid_generate_random)
using uuid_generate_time + uuid_generate_random we can get 32 bytes
unique id.
The string version of a 16 bytes uuid is 36 characters long (dashes included)

This note must be considered:
the field must store XID is defined as:
char data[XIDDATASIZE];
a char field can basically contain anything [0x00 - 0xff], but any programs
(Resource Managers) could encounter troblues in managing a binary content,
especially in tracing functions...
It's better we adopt a strict ASCII sequence, like the textual format 
generated by uuidgen.

******************************************

TRANSACTIONS AND RESOURCE MANAGERS

xa_start() and xa_end() allows an Application Program to associate/dissociate
a resource manager to/from a transaction. This can be done many times
during transaction lifecycle.
Resource Manager status and Transaction status must be modeled as two
distinct entities to allow association creation/deletion.
In Transaction Manager status file we need at least:
1. block for transaction status
N. blocks for resource manager status (related to the transaction)
TX_* specification does not allow a thread of control to manage more than
one transaction at a time, so for every thread of control we have only
1 transaction and N resource managers.

tx_begin:
1. generate XID
2. send to lixad the XID and the list of resource manager does not support
   dynamic registration
3. issue xa_start to the resouce manager of the above list
4. send to lixad the results of the xa_start
