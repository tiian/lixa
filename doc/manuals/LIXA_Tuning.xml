<chapter xml:id="Tuning"
	 xmlns:xlink="http://www.w3.org/1999/xlink">
  <title>Tuning</title>
  <section>
    <title>Introduction</title>
    <para>
      This chapter contains imformation useful to obtain the best performance
      of the LIXA state server (<command>lixad</command>) in your environment.
      As explained in <xref linkend="Configuring_the_server"/> the server
      provide some configuration parameters that can be tuned.
    </para>
    <para>
      There are basically two tuning paths:
      <itemizedlist mark='bullet'>
	<listitem><para>
	  number of server threads: the level of internal parallelism can be
	  tuned according with the number of available CPUs
	</para></listitem>
	<listitem><para>
	  disk performance: several parameters are available to find the best
	  trade off between performance and reliability
	</para></listitem>
      </itemizedlist>
      The two tuning paths are typically not fully independent: increasing the
      number of server threads increases the pressure to the disk(s) and vice
      versa.
    </para>
    <section>
      <title>Number of server threads</title>
      <para>
	The LIXA state server is a multi-threaded process with one network
	listener and many <quote>managers</quote>; every manager runs
	in a dedicated thread. Choosing the optimal number of threads requires
	some trials: following the <quote>just work</quote> concept
	the default configuration specifies 3 threads, but your 
	installation might perform better using a different number (in the
	below paragraphs you can collect some hints).
      </para>
      <note><para>
	If you use the <emphasis>journal based</emphasis> state engine, 
	<command>lixad</command> will start 3 different POSIX threads for every
	server thread; if you use the <emphasis>traditional</emphasis> state
	engine, <command>lixad</command> will start a single POSIX thread for
	every server thread.
      </para></note>
      <para>
	Refer to <xref linkend="Configuring_the_server"/> for information about
	how to specify the number of server threads.
      </para>
    </section>
    <section>
      <title>Disk performance</title>
      <para>
	To get the best IO performance, <command>lixad</command> provides 3
	complementary strategies:
	<itemizedlist mark='bullet'>
	  <listitem><para>
	    scattering IO through different disks: every server thread points
	    its own set of state files, if your system provides independent
	    disks, you can assign them to different server threads
	  </para></listitem>
	  <listitem><para>
	    choosing the state engine type: two state engines,
	    <emphasis>traditional</emphasis> and
	    <emphasis>journal based</emphasis>; the first has
	    a proven track of reliability because it has been deployed in
	    production environments since 2009 while the latter is a new
	    engine released in 2020 and provides amazingly low latency
	  </para></listitem>
	  <listitem><para>
	    configuring the state engine: depending on the previous choice,
	    different options are available to optimize the performance of
	    the state engine
	  </para></listitem>
	</itemizedlist>

      </para>
    </section>
  </section>
  <section>
    <title>@@@ Disk performance</title>
      <para>
	As explained in <xref linkend="Configuring_the_server"/> every
	manager inside the LIXA state server uses a specific path for
	its status files. If you specified path associated to independent
	disks, you should obtain the best I/O performance for the LIXA
	state server.
      </para>
      <para>
	Following the <quote>just work</quote> concept the default 
	configuration specifies a path like
	<filename>/opt/lixa/var/...</filename>
	but you should change it if your system had indipendent disks.
      </para>
    </section>
    <section>
      <title>Minimum and maximum elapsed synchronization time</title>
      <para>
	Starting with version 0.7.2 you can configure two parameters:
	<parameter>min_elapsed_sync_time</parameter> and 
	<parameter>max_elapsed_sync_time</parameter>. 
	In the previous versions these parameters was implicitly set to 0.
      </para>
      <para>
	Setting them to <quote>0</quote> will dramatically reduce the
	probability a synchronization operation can be shared by two or more
	client sessions (LIXA transaction managers). Setting them to a value
	greater than zero
	will increase the likelihood a synchronization operation
	batches a lot of requests.
      </para>
      <important>
	<para>
	  The higher the value of these parameters, the higher the chance
	  you will have to perform manual recovery in the case of a server
	  crash (manual recovery is explained in 
	  <xref linkend="Manual_cold_recovery"/>).
	</para>
	<para>
	  <emphasis>Do not use too high values:</emphasis> you will increase
	  the likelihood of a long and tiring manual recovery phase after
	  a server crash without extra performance benefits.
	</para>
      </important>
  </section>
  <section xml:id="Tuning_example">
    <title>A tuning example</title>
    <para>
      This section explains a tuning example you can use as a starting point
      to develop your own tuning strategy.
    </para>
    <para>
      The utility program <command>lixat</command>, introduced in
      <xref linkend="Starting_test_utility"/> can be used as a benchmark tool
      specifying <parameter>-b</parameter> 
      (<parameter>--benchmark</parameter>) option.
      The available command options can be retrieved with
      <parameter>--help</parameter>:
      <screen>
tiian@ubuntu:~$ /opt/lixa/bin/lixat --help
Usage:
  lixat [OPTION...] - LIXA test utility

Help Options:
  -?, --help                  Show help options

Application Options:
  -c, --commit                Perform a commit transaction
  -r, --rollback              Perform a rollback transaction
  -v, --version               Print package info and exit
  -b, --benchmark             Perform benchmark execution
  -o, --open-close            Execute tx_open &amp; tx_close for every transaction [benchmark only]
  -s, --csv                   Send result to stdout using CSV format [benchmark only]
  -l, --clients               Number of clients (threads) will stress the state server [benchmark only]
  -d, --medium-delay          Medium (random) delay between TX functions [benchmark only]
  -D, --delta-delay           Delta (random) delay between TX functions [benchmark only]
  -p, --medium-processing     Medium (random) delay introduced by Resource Managers operations between tx_begin and tx_commit/tx_rollback [benchmark only]
  -P, --delta-processing      Delta (random) delay introduced by Resource Managers operations between tx_begin and tx_commit/tx_rollback [benchmark only]
      </screen>
      These are the interesting options in benchmark mode:
      <itemizedlist mark='bullet'>
	<listitem><para>
	    commit transactions (<parameter>-c</parameter> or
	    <parameter>--commit</parameter>)
	</para></listitem>
	<listitem><para>
	    rollback transactions (<parameter>-r</parameter> or
	    <parameter>--rollback</parameter>)
	</para></listitem>
	<listitem><para>
	    one couple of <function>tx_open()/tx_close()</function> for every
	    transaction (<parameter>-o</parameter> or
	    <parameter>--open-close</parameter>); alternatively only one
	    couple of <function>tx_open()/tx_close()</function> will be used
	    for all the transactions (<function>tx_open()/tx_begin()/tx_commit()/tx_begin()/tx_commit()/.../tx_close()</function>)
	</para></listitem>
	<listitem><para>
	    number of clients connected to the LIXA state server
	    (<parameter>-l</parameter> or <parameter>--clients</parameter>)
	</para></listitem>
	<listitem><para>
	    delay introduced by Application Program logic between 
	    <function>tx_*</function> functions (
	    <parameter>-d, --medium-delay, -D, --delta-delay</parameter>)
	</para></listitem>
	<listitem><para>
	    delay introduced by Resource Managers logic between
	    <function>tx_begin</function> and <function>tx_commit</function>
	    (or <function>tx_rollback</function>) functions (
	    <parameter>-p, --medium-processing, -P, --delta-processing</parameter>)
	</para></listitem>
      </itemizedlist>
    </para>
    <section>
      <title><command>lixat</command> benchmark behavior</title>
      <para>
	This is a sketch of <command>lixat</command> algorithm when
	<parameter>-o</parameter> or <parameter>--open-close</parameter>
	option is specified:
	<screen>
loop (1..100)
    sleep(random[d-D/2, d+D/2])
    tx_open()
    sleep(random[d-D/2, d+D/2])
    tx_begin()
    sleep(random[p-P/2, p+P/2])
    tx_commit()
    sleep(random[d-D/2, d+D/2])
    tx_close()
    sleep(random[d-D/2, d+D/2])
end loop
	</screen>
	With default delays the sleeping pauses are the following:
	<screen>
sleep(random[d-D/2, d+D/2]) --&gt; [500, 1500] microseconds
sleep(random[p-P/2, p+P/2]) --&gt;  [50, 150]  milliseconds	  
	</screen>
        without <parameter>-o</parameter> or 
	<parameter>--open-close</parameter> option <command>lixat</command>
	does not
	call <function>tx_open()/tx_close()</function> for every cycle and
	the algorithm becomes the following one:
	<screen>
tx_open()
loop (1..100)
    sleep(random[d-D/2, d+D/2])
    sleep(random[d-D/2, d+D/2])
    tx_begin()
    sleep(random[p-P/2, p+P/2])
    tx_commit()
    sleep(random[d-D/2, d+D/2])
    sleep(random[d-D/2, d+D/2])
end loop
tx_close()
	</screen>
      </para>
      <para>
	Using <parameter>--open-close</parameter> parameter you will simulate 
	an Application Program that creates and destroy the transactional
	environment for every transaction.
      </para>
      <para>
	Omitting <parameter>--open-close</parameter> parameter you will 
	simulate an Application Program that reuses the transactional
	environment for 100 transactions.
      </para>
      <para>
	Your Application Program could live in the middle, 
	<command>lixat</command> can help you to figure out two different 
	theoretical scenarios.
      </para>
      <para>
	A shell command you can use to measure the performance of LIXA for
	10, 20, 30, ... 100 clients is the following one:
	<screen>
for l in 10 20 30 40 50 60 70 80 90 100 ; do /opt/lixa/bin/lixat -b -s -l $l ; done | grep -v '^ ' > /tmp/bench_result.csv
	</screen>
      </para>
    </section>
    <section>
      <title>Tuning example hardware characteristics</title>
      <para>
	This is the output of <filename>/proc/cpuinfo</filename> executed
	in the system hosting <command>lixad</command> state server:
	<screen>
	  processor: 0
	  vendor_id: GenuineIntel
	  cpu family: 15
	  model: 2
	  model name: Intel(R) Celeron(R) CPU 2.40GHz
	  stepping: 9
	  cpu MHz: 2405.521
	  cache size: 128 KB
	  fdiv_bug: no
	  hlt_bug: no
	  f00f_bug: no
	  coma_bug: no
	  fpu: yes
	  fpu_exception: yes
	  cpuid level: 2
	  wp: yes
	  flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe up pebs bts cid xtpr
	  bogomips: 4811.04
	  clflush size: 64
	</screen>
	This is the output of <filename>/proc/cpuinfo</filename> executed
	in the system hosting <command>lixat</command> benchmark process:
	<screen>
	  processor: 0
	  vendor_id: GenuineIntel
	  cpu family: 6
	  model: 23
	  model name: Genuine Intel(R) CPU           U7300  @ 1.30GHz
	  stepping: 10
	  cpu MHz: 800.000
	  cache size: 3072 KB
	  physical id: 0
	  siblings: 2
	  core id: 0
	  cpu cores: 2
	  apicid: 0
	  initial apicid: 0
	  fpu: yes
	  fpu_exception: yes
	  cpuid level: 13
	  wp: yes
	  flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx lm constant_tsc arch_perfmon pebs bts rep_good aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm sse4_1 xsave lahf_lm tpr_shadow vnmi flexpriority
	  bogomips: 2593.73
	  clflush size: 64
	  cache_alignment: 64
	  address sizes: 36 bits physical, 48 bits virtual
	  power management:

	  processor: 1
	  vendor_id: GenuineIntel
	  cpu family: 6
	  model: 23
	  model name: Genuine Intel(R) CPU           U7300  @ 1.30GHz
	  stepping: 10
	  cpu MHz: 800.000
	  cache size: 3072 KB
	  physical id: 0
	  siblings: 2
	  core id: 1
	  cpu cores: 2
	  apicid: 1
	  initial apicid: 1
	  fpu: yes
	  fpu_exception: yes
	  cpuid level: 13
	  wp: yes
	  flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx lm constant_tsc arch_perfmon pebs bts rep_good aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm sse4_1 xsave lahf_lm tpr_shadow vnmi flexpriority
	  bogomips: 2593.50
	  clflush size: 64
	  cache_alignment: 64
	  address sizes: 36 bits physical, 48 bits virtual
	  power management:
	</screen>
      </para>
      <para>
	From the above specs you can guess these are not powerful 
	<quote>server systems</quote>. The systems are connected with a
	100 Mbit/s connection with an average latency of 95 microseconds:
	<screen>
--- 192.168.10.2 ping statistics ---
50 packets transmitted, 50 received, 0% packet loss, time 48997ms
rtt min/avg/max/mdev = 0.133/0.190/0.226/0.024 ms
	</screen>
      </para>
    </section>
    <section>
      <title>Results obtained with <parameter>--open-close</parameter> parameter</title>
      <note>
	<para>
	  All the tests saturated the CPU of the host executing 
	  <command>lixad</command> state server for the higher values of
	  connected clients.
	</para>
      </note>
      <para>
	The first picture shows the elapsed time associated to
	<function>tx_open()</function> increases quite linearly with the 
	number of connected clients. A <command>lixad</command> state server 
	configured with 3 managers
	(threads), <parameter>min_elapsed_sync_time=20</parameter> and
	<parameter>max_elapsed_sync_time=100</parameter> exploits the best
	scalability (purple line); a <command>lixad</command> state server 
	configured with 3 managers,
	<parameter>min_elapsed_sync_time=10</parameter> and
	<parameter>max_elapsed_sync_time=50</parameter> shows a scalability
	very near to the best (light green line).
	<emphasis>The second one is a more robust configuration and should
	  be preferred</emphasis>.
      </para>
      <figure xml:id="tuning_01">
	<title>Elapsed time of tx_open() when the Application Program 
	  uses a couple of tx_open()/tx_close() for every 
	  couple of tx_begin()/tx_commit()</title>
	<mediaobject>
	  <imageobject>
	    <imagedata fileref="../images/LIXA_Tuning_01.png"/>
	  </imageobject>
	</mediaobject>
      </figure>
      <para>
	A completely different behavior is shown by
	<function>tx_begin(), tx_commit(), tx_close()</function> functions:
	the best scalability is obtained with 1 manager (thread), 
	<parameter>min_elapsed_sync_time=20</parameter> and
	<parameter>max_elapsed_sync_time=100</parameter>
	(yellow line); a quite optimal
	performance can be obtained with 1 manager (thread), 
	<parameter>min_elapsed_sync_time=10</parameter> and
	<parameter>max_elapsed_sync_time=50</parameter> (orange line). 
	The second 
	configuration should be preferred because it's more robust than the
	first one. Using more threads does not give any benefit for these
	three functions.
      </para>
      <figure xml:id="tuning_02">
	<title>Elapsed time of tx_begin() when the Application Program 
	  uses a couple of tx_open()/tx_close() for every 
	  couple of tx_begin()/tx_commit()</title>
	<mediaobject>
	  <imageobject>
	    <imagedata fileref="../images/LIXA_Tuning_02.png"/>
	  </imageobject>
	</mediaobject>
      </figure>
      <figure xml:id="tuning_03">
	<title>Elapsed time of tx_commit() when the Application Program 
	  uses a couple of tx_open()/tx_close() for every 
	  couple of tx_begin()/tx_commit()</title>
	<mediaobject>
	  <imageobject>
	    <imagedata fileref="../images/LIXA_Tuning_03.png"/>
	  </imageobject>
	</mediaobject>
      </figure>
      <figure xml:id="tuning_04">
	<title>Elapsed time of tx_close() when the Application Program 
	  uses a couple of tx_open()/tx_close() for every 
	  couple of tx_begin()/tx_commit()</title>
	<mediaobject>
	  <imageobject>
	    <imagedata fileref="../images/LIXA_Tuning_04.png"/>
	  </imageobject>
	</mediaobject>
      </figure>
      <para>
	In the last chart 
	you may note the aggregated values for all the transactions
	(100 transactions, the elapsed time is now expressed in seconds):
	the purple line (best configuration for <function>tx_open()</function>)
	is the best overall configuration. This time too, there are many
	performance equivalent configurations:
	<itemizedlist mark='bullet'>
	  <listitem><para>
	      3 managers (thread), 
	      <parameter>min_elapsed_sync_time=20</parameter> and
	      <parameter>max_elapsed_sync_time=100</parameter>
	      (purple line)
	  </para></listitem>
	  <listitem><para>
	      3 managers (thread), 
	      <parameter>min_elapsed_sync_time=10</parameter> and
	      <parameter>max_elapsed_sync_time=50</parameter>	      
	      (light green line)
	  </para></listitem>
	  <listitem><para>
	      2 managers (thread), 
	      <parameter>min_elapsed_sync_time=10</parameter> and
	      <parameter>max_elapsed_sync_time=50</parameter>	      
	      (red line)
	  </para></listitem>
	  <listitem><para>
	      1 managers (thread), 
	      <parameter>min_elapsed_sync_time=10</parameter> and
	      <parameter>max_elapsed_sync_time=50</parameter>	      
	      (orange line)
	  </para></listitem>
	</itemizedlist>
	The second configuration of the above list (light green) could be 
	considered the
	best from an overall performance point of view and from a safety
	point of view: the minimum elapsed synchronization time is 
	10 milliseconds.
      </para>
      <figure xml:id="tuning_05">
	<title>Overall elapsed time when the Application Program 
	  uses a couple of tx_open()/tx_close() for every 
	  couple of tx_begin()/tx_commit()</title>
	<mediaobject>
	  <imageobject>
	    <imagedata fileref="../images/LIXA_Tuning_05.png"/>
	  </imageobject>
	</mediaobject>
      </figure>
    </section>
    <section>
      <title>Results obtained without <parameter>--open-close</parameter> parameter</title>
      <note>
	<para>
	  All the tests saturated the CPU of the host executing 
	  <command>lixad</command> state server for the higher values of
	  connected clients.
	</para>
      </note>
      <para>
	Avoiding a lot of <function>tx_open()/tx_close()</function> the
	behavior of the system is quite different. It's interesting to note
	the system has two distinct modes:
	<itemizedlist mark='bullet'>
	  <listitem><para>
	      in the range [10,50] clients the scalability is quite linear 
	      if you adopt a super safe configuration with 
	      <parameter>min_elapsed_sync_time=0</parameter> and
	      <parameter>max_elapsed_sync_time=0</parameter>
	  </para></listitem>
	  <listitem><para>
	      in the range [10,50] clients the scalability is
	      <quote>superlinear</quote>
	      <footnote>
		<para>
		  <function>tx_begin(), tx_commit(), tx_close()</function>
		  response times change very few when the number of clients
		  rises
		</para>
	      </footnote>
	      if you adopt an asynchronous
	      conifiguration with 
	      <parameter>min_elapsed_sync_time=10</parameter> and
	      <parameter>max_elapsed_sync_time=50</parameter>
	      or higher values
	  </para></listitem>
	  <listitem><para>
	      in the range [60,100] clients the system tends to saturate
	      and the <quote>superlinear</quote> characteristic is vanishing;
	      neverthless, asynchronous configurations exploit lower
	      response time than synchronous ones
	  </para></listitem>
	</itemizedlist>
      </para>
      <figure xml:id="tuning_06">
	<title>Elapsed time of tx_open() when the Application Program 
	  uses a couple of tx_open()/tx_close() for a 
	  batch of tx_begin()/tx_commit()</title>
	<mediaobject>
	  <imageobject>
	    <imagedata fileref="../images/LIXA_Tuning_06.png"/>
	  </imageobject>
	</mediaobject>
      </figure>
      <figure xml:id="tuning_07">
	<title>Elapsed time of tx_begin() when the Application Program 
	  uses a couple of tx_open()/tx_close() for a 
	  batch of tx_begin()/tx_commit()</title>
	<mediaobject>
	  <imageobject>
	    <imagedata fileref="../images/LIXA_Tuning_07.png"/>
	  </imageobject>
	</mediaobject>
      </figure>
      <figure xml:id="tuning_08">
	<title>Elapsed time of tx_commit() when the Application Program 
	  uses a couple of tx_open()/tx_close() for a 
	  batch of tx_begin()/tx_commit()</title>
	<mediaobject>
	  <imageobject>
	    <imagedata fileref="../images/LIXA_Tuning_08.png"/>
	  </imageobject>
	</mediaobject>
      </figure>
      <figure xml:id="tuning_09">
	<title>Elapsed time of tx_close() when the Application Program 
	  uses a couple of tx_open()/tx_close() for a 
	  batch of tx_begin()/tx_commit()</title>
	<mediaobject>
	  <imageobject>
	    <imagedata fileref="../images/LIXA_Tuning_09.png"/>
	  </imageobject>
	</mediaobject>
      </figure>
      <para>
	The last chart shows the average elapsed time spent for 
	<function>tx_*</function> functions by 100 transactions.
	The best performace is obtained with the less safe configuration:
	<parameter>min_elapsed_sync_time=20</parameter> and
	<parameter>max_elapsed_sync_time=100</parameter>
	(purple, cyan and yellow lines).
	The intermediate performance is obtained with the intermediate
	configuration:
	<parameter>min_elapsed_sync_time=10</parameter> and
	<parameter>max_elapsed_sync_time=50</parameter>
	(dark yellow, red and orange lines).
	The worst performance is obtained with the safest configuration:
	<parameter>min_elapsed_sync_time=0</parameter> and
	<parameter>max_elapsed_sync_time=0</parameter>
	(green, light green and blue lines).
      </para>
      <figure xml:id="tuning_10">
	<title>Overall elapsed time when the Application Program 
	  uses a couple of tx_open()/tx_close() for a 
	  batch of tx_begin()/tx_commit()</title>
	<mediaobject>
	  <imageobject>
	    <imagedata fileref="../images/LIXA_Tuning_10.png"/>
	  </imageobject>
	</mediaobject>
      </figure>
      <section>
	<title>Conclusions</title>
	<para>
	  The LIXA project gives you some parameters you can change to tune
	  your installation and get the best performance.
	  There is not a magic recipe you can adopt, but there are some
	  rules of thumb:
	  <itemizedlist mark='bullet'>
	    <listitem><para>
		if possible, avoid the usage of 
		<function>tx_open()/tx_close()</function> for every 
		transaction: if your business logic could batch more than
		one transaction inside the same session, your overall
		response time would be lower
	    </para></listitem>
	    <listitem><para>
		delayed disk synchronization will help you in obtaining
		better performance with the same hardware, but introducing
		too high delays will not give you extra performance
	    </para></listitem>
	    <listitem><para>
		delay disk synchronization introduce the risk to perform
		manual recovery as explained in
		<xref linkend="Delayed_synchronization_effects"/>.
	    </para></listitem>
	  </itemizedlist>
	  Write your own test program, with real Resource Managers,
	  and measure it: with a test environment you would be able to
	  fine tune your own installation.
	</para>
      </section>
    </section>
  </section>
</chapter>
